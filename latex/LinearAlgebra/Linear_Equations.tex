\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{physics}
\usepackage[english]{babel}
\usepackage[margin=2cm]{geometry}
\usepackage{mathrsfs}  
\usepackage{graphicx}
\usepackage{float} %use the option [H]
\graphicspath{ {images/} }
\usepackage{amsthm} %lets us use \begin{proof}
\usepackage{amsmath}
\DeclareMathOperator{\arcsinh}{arcsinh}
\usepackage{amssymb} %gives us the character
\usepackage{CJKutf8}
\usepackage[export]{adjustbox}
\setlength{\parindent}{0cm} % starting spaces
\setlength{\parskip}{1em} % paragraph width
\usepackage{array}
\usepackage{tabularx}
\usepackage{markdown}
\title{\textbf{Linear Algebra Notes}}
\author{Book: Linear Algebra, by HOFFMAN,KUNZE}
\date{\today}
\begin{document}
\begin{CJK*}{UTF8}{bsmi}
\linespread{1.5}
\maketitle
\newcommand{\st}[1]{\section*{#1}}
\newcommand{\sst}[1]{\subsection*{#1}}
\newcommand{\ssst}[1]{\subsubsection*{#1}}
\newcommand{\dsp}{\displaystyle}
\newcommand{\dx}{\,dx}
\newcommand{\dphi}{\,d\phi}
\newcommand{\dtheta}{\,d\theta}
\newcommand{\dy}{\,dy}
\newcommand{\dz}{\,dz}
\newcommand{\dr}{\,dr}
\newcommand{\drho}{\,d\rho}
\newcommand{\tb}{\textbf}
\newcommand{\scr}{\mathscr}
\st{Chapter 1.}
\sst{Definition list}
\begin{itemize}
    \item If A and B are m X n matrices over the field \(F\), we say that B is row-equivalent to A if B can be obtained from A by a finite sequence of elementary row operation.
    \item An m X n matrix R is called row-reduced if the first non-zero entry in each non-zero row of R is equal to 1 and each column of R which contains the leading non-zero entry of some row has all its other entries 0.
    \item An m X n matrix R is called a row-reduced echelon matrix if:\begin{enumerate}
        \item R is row-reduced
        \item every row of R which has all its entries 0 occurs below every row which has a non-zero entry
        \item if row1,\(\ldots\),r are the non-zeros rows of R, and if the leading non-zero entry of row i occurs in column \(k_i\), i =1,\(\ldots\),r, then \(k_1<k_2<\cdots<k_r\)
    \end{enumerate}
    \item Let A be an (m,n) matrix over the field \(F\) and let B be an (n,p) matrix over \(F\). The product AB is the (m,p) matrix C whose i,j entry is\begin{equation}
        C_{ij} = \sum_{r=1}^n A_{ir}B_{rj} \implies C_{i*} = \sum^r A_{ir}B_{r*},\, C_{*j}=\sum^r A_{*r}B_{rj} 
    \end{equation}
    \item An (m,n) matrix is said to be an elementary matrix if it can be obtained from the (m,m) identity matrix by means of a single elementary row operation.
    \item Let A be an (n,n) matrix over the field \(F\). An (n,n) matrix B such that \(BA=I\) is called a \emph{left inverse} of A\@ an (n,n) matrix B such that \(AB=I\) is called a \emph{right inverse} of A. \(If AB=BA=I\), then B is called a \emph{two-sided inverse} of A and A is said to be \emph{invertible}.
    
    \tb{Lemma.} \(If\) A has a left inverse B and a right inverse C, then \(B=C\).
\end{itemize}
\sst{Theorem list}
\begin{enumerate}
    \item Equivalent systems of linear equations have exactly the same solutions.
    \item To each elementary row operation \(e\) there corresponds an elementary row operation \(e_1\) of the same type as \(e\), s.t. \(e_1(e(A))=A\).
    \item If A and B are row-equivalent m X n matrices, the homogeneous systems of linear equations \(AX=0\) and \(BX=0\) have exactly the same solutions.
    \item Every m X n matrix over  the field \(F\) is row-equivalent to a row-reduced matrix.
    \item Every m X n matrix over  the field \(F\) is row-equivalent to a row-reduced echelon matrix.
    \item If A is an m X n matrix and \(m<n\), then the homogeneous system of linear equations \(AX=0\) has a non-trivial solution.
    \item If A is an n X N matrix, then A is row-equivalent to the identity matrix if and only if the system of equations \(AX=0\) has only the trivial solution.
    \item If A,B,C are matrices over the field \(F\) such that the products BC and A (BC) are defined, then so are the products AB, (AB) C and\begin{equation}
        A(BC) = (AB)C
    \end{equation}
    \item Let \(e\) be an elementary row operation and let E be the (m,m) elementary matrix \(E=e(I)\). Then, for every (m,n) matrix A,\begin{equation}
        e(A) = EA
    \end{equation}
    \tb{Corollary.}
    Let A and B be (m,n) matrices over the field \(F\). Then B is row-equivalent to A if and only if \(B=PA\), where P is a product of (m,m) elementary matrices.
    \item Let A and B be (n,n) matrices over \(F\).\begin{enumerate}
        \item \(If\) A is invertible, so is \(A^{-1}\) and \({(A^{-1})}^{-1}=A\).
        \item \(If\) both A and B are invertible, so is AB, and \({(AB)}^{-1} = B^{-1}A^{-1}\).
    \end{enumerate}
    \tb{Corollary.}
    A product of invertible matrices is invertible.
    \item An elementary matrix is invertible.
    \item \(If\) A is an (n,n) matrix, the following are equivalent.\begin{enumerate}
        \item A is invertible.
        \item A is row-equivalent to the (n,n) identity matrix.
        \item A is a product of elementary matrices.
    \end{enumerate}
    \tb{Corollary.}
    \(If\) A is an invertible (n,n) matrix and if a sequence of elementary row operations reduces A to the identity, then that same sequence of operations when applied to I yields \(A^{-1}\).
    
    \tb{Corollary.}
    Let A and B be (m,n) matrices. Then B is row-equivalent to A \emph{if and only if} \(B=PA\) where P is an invertible (m,m) matrix.
    \item For an (n,n) matrix A, the following are equivalent.\begin{enumerate}
        \item A is invertible.
        \item The homogeneous system \(AX=0\) has only the trivial solution \(X=0\).
        \item The system of equations \(AX=Y\) has a solution X for each (n,1) matrix Y.
    \end{enumerate}
    \tb{Corollary.} A square matrix with either a left or right inverse is invertible.

    \tb{Corollary.} Let \(A = A_1A_2\cdots A_k\), where \(A_1\ldots,A_k\) are (n,n) matrices. Then A is invertibl \emph{if and only if } each \(A_j\) is invertible.
\end{enumerate}
\st{Chapter 2.}
\sst{Old definition list}
\begin{itemize}
    \item A \tb{vector space} consists of the following:\begin{enumerate}
        \item a field F of scalars
        \item a set V of objects, called vectors
        \item a rule (or operation), called vector addition, which associates with each pair of vectors \(\alpha,\, \beta\) in V a vector \(\alpha+\beta\) in V, called the sum of \(\alpha\) and \(\beta\), in such a way thay\begin{enumerate}
            \item addition is commutative, \(\alpha+\beta = \beta+\alpha\)
            \item addition is associativ, \(\alpha+ (\beta+\gamma) = (\alpha+\beta)+\gamma \) 
            \item there is a unique vector 0 in V, called the zero vector, such that \(\alpha+0=\alpha\) for all \(\alpha\) in V
            \item for each vector \(\alpha\) in V there is a unique vector \(-\alpha\) in V such that \(\alpha +(-\alpha)=0\)
        \end{enumerate}
        \item a rule (or operation), called scalar multiplication, which associates with each scalar c in F and vector \(\alpha\) in V, called the product of \(c\) and \(\alpha\), in such a way thay\begin{enumerate}
            \item \(1\alpha = \alpha\)
            \item \((c_1c_2)\alpha=c_1(c_2\alpha)\)
            \item \(c(\alpha+\beta)=c\alpha+c\beta\)
            \item \((c_1+c_2)\alpha = c_1\alpha+c_2\alpha\)
        \end{enumerate}
    \end{enumerate}
    \item A vector \(\beta\) in V is said to be a \tb{linear combination} of the vectors \(\alpha_1,\ldots,\alpha_n\) in V provided there exist scalars \(c_1,\ldots,c_n\) in F such that \begin{equation}
        \beta = \sum_{i=1}^n c_i\alpha_i
    \end{equation}
    \item Let V be a vector space over the field F. A \tb{subspace} of V is a subset W of V which is itself a vector space over F with the operations of vector addition and scalar multiplication on V.
    \item Let S be a set of vectors in a vector space V. The \tb{subspace spanned} by S is defined to be the intersection W of all subspaces of V which contain S. When S is a finite set of vectors, \(S=\{\alpha_1,\,\alpha_2,\ldots,\alpha_n\}\), we shall simply call W the subspace spanned by the vectors \(\alpha_1,\,\alpha_2,\ldots,\,\alpha_n\).
    \item \(If\) \(S_1,S_2,\ldots,S_k\) are subsets of a vector space V, the set of all sums\begin{equation}
        \alpha_1+\alpha_2+\cdots+\alpha_k
    \end{equation}
    of vectors \(\alpha_i\) in \(S_i\) is called the \tb{sum} of the subsets \(S_1,\,S_2,\ldots,\,S_k\) and is denoted by\begin{equation}
        \sum_{i=1}^k S_i
    \end{equation}
    If \(W_1,\,W_2,\ldots,\,W_k\) are subspaces of V, then the sum\begin{equation}
        W = W_1+W_2+\cdots+W_k
    \end{equation}
    is easily seen to be a subspace of V which contains each of the subspaces \(W_i\). From this it follows, as in the proof of Theorem 3, that \(W\) is the subspace spanned by the union of \(W_1,\,W_2,\ldots,\,W_k\)
\end{itemize}
\st{Definition list}
\sst{Linearly dependnet}
Let V be a vector space over F. A subset S of V is said to be \tb{linearly dependent} if there exist distinct vectors \(\alpha_1,\,\alpha_2,\ldots,\,\alpha_n\) in S and scalars \(c_1,\,c_2,\ldots,\,c_n\) in F, not all of which are 0, such that \begin{equation}
    \sum_{i=1}^n c_i\alpha_i=0
\end{equation}
A set which is not linearly dependent is called \tb{linearly independent}. If the set S contains only finitely many vectors \(\alpha_1,\,\alpha_2,\ldots,\,\alpha_n\), we sometimes say that the vectors are dependent instead of saying S is dependent.

\sst{Basis}
Let V be a vector space. A \tb{basis} for V is a linearly independent set of vectors in V which spans the space V. The space V is finite-dimensional if it has a finite basis.
\sst{Ordered basis}
\(If\) V is a finite-dimensional vector space, an ordered basis for V is a finite sequence of vectors which is linearly independent and spans V.
\st{Theorem list}
\begin{enumerate}
    \item A non-empty subset W of V is a subspace of V \emph{if and only if} for each pair of vectors \(\alpha,\,\beta\) in W and each scalar c in F the vector \(c\alpha+\beta\) is again in W.
    \item Let V be a vector space over the field F. The intersection of any collection of subspace of V is a subspace of V.
    \item The subspace spanned by a non-empty subset S of a vector space V is the set of all linear combinations of vectors in S.
    \item Let V be a vector space which is spanned by \(\beta_1,\ldots,\beta_m\). Then any independent set of vectors in V is finite and contains no more tahn m elements.
    
    \tb{Corollary} \(If\) V is a finite-dimensional vector space, then any two bases of V have same number of elements.
    
    \tb{Corollary} Let V be a finite-dimensional vector space and let \(n = \text{dim}\,V\). Then\begin{itemize}
        \item any subset of V which contains more than n vectors is linearly dependent
        \item no subset of V which contains fewer than n vectors can span V.
    \end{itemize}
    \tb{Lemma.} Let S be a linearly independent subset of a vector space V. Suppose \(\beta\) is a vector in V which is not in the subspace spanned by S. Then the set obtained by adjoining \(\beta\) to S is linearly independent.
    \item \(If\) W is a subspace of a finite-dimensional vector space V, every linearly independent subset of W is finite and is part of a finite basis for W.
    
    \tb{Corollary} If W is a proper subspace of a finite-dimensional vector space V, then W is finite-dimensional and dim W \(<\) dim V

    \tb{Corollary} In a finite-dimensional vector space V every non-empty linearly independent set of vectors is part of a basis.

    \tb{Corollary} Let A be an (n,n) matrix over a field F, and suppose the row vectors of A form a linearly independent set of vectors in \(F^n\). Then A is invertible.
    \item If \(W_1\) and \(W_2\) are finite-dimensional subspaces of a vector space V, then \(W_1+W_2\) is finite dimensional and \begin{equation}
        \dim W_1 + \dim W_2 = \dim{(W_1 \cap W_2)} + \dim(W_1+W_2) 
    \end{equation}
    \item Let V be an n-dimensional vector space over the field F, and let \(\mathscr{B}\) and \(\mathscr{B}^\prime\) be two ordered bases of V. Then there is a unique, necessarily invertible, (n,n) matrix P with entries in F such that\begin{align}
        &[\alpha]_\mathscr{B} = P [\alpha]_{\mathscr{B^\prime}}\\
        &[\alpha]_\mathscr{B^\prime} = P^{-1} [\alpha]_{\mathscr{B}}
    \end{align}
    where \(\alpha_j^\prime =\sum\limits_{i=1}^n P_{ij} \alpha_i\).The columns of P are given by\begin{equation}
        P_j = [\alpha_j^\prime]_{\mathscr{B}}, \; j=1,\ldots,n.
    \end{equation}
    \item Suppose P is an (n,n) invertible matrix over F and \(\mathscr{B}\) be an ordered basis of V. Then there is a unique ordered \(\mathscr{B}\) of V such that \begin{align}
        &[\alpha]_{\mathscr{B}} = P[\alpha]_{\mathscr{B^\prime}}\\
        &[\alpha]_{\mathscr{B^{\prime}}} = P^{-1}[\alpha]_{\mathscr{B}}
    \end{align}
    \item Row-equivalent matrices have the same row space.
    \item Let R be a non-zero row-reduced echelon matrix. Then the non-zero row vectors of R form a basis for the row space of R. 
    \item Let m and n be postive integers and lef F be a field. Suppose W is a subspace of \(F^n\) and \(\dim W\leq m\). Then there is precisely one (m,n) row-reduced echelon 
\end{enumerate}
\st{Chapter 3.}
\st{Theorem list}
\begin{enumerate}
    \item Let V be a finite-dimensional vector space over the field F and let \(\{\alpha_1,\ldots,\alpha_n\}\) be an ordered basis for V. Let W be a vector space over the same field F and let \(\beta_1,\ldots,\beta_n\) be any vectors in W. Then there is precisely one linear transforamtion T from V into W such that \begin{equation}
        T\alpha_j = \beta_j,\;\;j=1,\ldots,n.
    \end{equation}
    \item Let V and W be vector space over the field F and let T be a linear transforamtion from V into W. Suppose that V is finite-dimensional.\begin{equation}
        \text{rank}(T)+ \text{nullity}(T) = \dim V.
    \end{equation}
    \item \(If\) A is an (m,n) matrix with entries in the field F, then\begin{equation}
        \text{row rank} (A) = \text{column rank}(A)
    \end{equation}
    \item T and U are linear transforamtion from V into W. Define\begin{align}
        &(T+U)(\alpha) = T\alpha+U\alpha\\
        &(cT)(\alpha) = c(T\alpha)
    \end{align}
    then, the set of all linear transforamtions from V into W, is a vector space over the field F.
    \item Let V be an n-dimensional vector space over the field F, and let W be an m-dimensional vector space over F. Then the space L(V,W) is finite-dimensional and has dimension mn.
    \item Let V, W, and Z be vector spaces over the field F. Let T be a linear transforamtion from V into W and U a linear transforamtion from W into Z. Then the composed function UT defined by (UT) (\(\alpha\)) = U(T(\(\alpha\))) is a linear transforamtion from V into Z.
    \item Let V and W be vector spaces over the field F and let T be a linear transforamtion from V into W. \(If\) T is invertible, then the inverse function \(T^{-1}\) is a linear transforamtion from W onto V.
    \item Let T be a linear transforamtion from V into W. Then T is non-singular if and only if T carries each linearly independent subset of V onto a linearly independent subset of W.
    \item Let V and W be finite-dimensional vector spaces over the field F such that \(\dim V = \dim W\). \(If\) T is a linear transforamtion from V into W, the following are equivalent:\begin{enumerate}
        \item T is invertible.
        \item T is non-singular.
        \item T is onto, that is, the range of T is W.
    \end{enumerate}
    \item Every n-dimensional vector space over tyhe field F is isomorphic to the space \(F^n\).
    \item Let V be an n-dimensional vector space over the field F and W an m-dimensional vector space over F. Let \(\scr{B}\) be an ordered basis for V and \(\scr{B^\prime}\) an ordered basis for W. For each linear transformation T from V into W, there is an (m,n) matrix A with entries in F such that\begin{eqnarray}
        {[T\alpha]}_{\scr{B^\prime}} = A{[\alpha]}_{\scr{B}}
    \end{eqnarray}
    for every vector \(\alpha\) in V. Furthermore, \(T\xrightarrow{}A\) is a one-one correspondence between the set of all linear transforamtions from V into W and the set of all (m,n) matrices over the field F.
    \item Let V be an n-dimensional vector space over the field F and let W be an m-dimensional vector space over F. For each pair of ordered bases \(\scr{B,\,B^\prime}\) for V and W respectively, the function which assigns to a linear transforamtion T its matrix relative to \(\scr{B,\,B^\prime}\) is an isomorphism between the space L(V,\,W) and the space of all (m,n) matrices over the field F.
    \item Let V, W, and Z be finite-dimensional vector spaces over the field F; let T be a linear transforamtion from V into W and U a linear transforamtion from W into Z. If \(\beta,\,\beta^\prime,\,\beta^{\prime\prime}\) are ordered bases for the space V, W, and Z, respectively, if A is the matrix of T relative to the pair \(\beta,\,\beta^\prime\), and B is the matrix of U relative to the pair \(\beta^\prime,\,\beta^{\prime\prime}\), then the matrix of the composition UT relative to the pair \(\beta,\,\beta^{\prime\prime}\) is the product matrix C=BA.
    \item Let V be a finite-dimensional vector space over the field F, and let\begin{equation}
        \beta = \{\alpha_1,\ldots,\,\alpha_n\}\quad \beta^\prime = \{\alpha_1^\prime,\ldots,\,\alpha_n^\prime\}
    \end{equation}
    be ordered bases for V. Suppose T is a linear operator on V. If \(P=[P_1,\ldots,\,P_n]\) is the (n,n) matrix with columns \(P_j = [\alpha^\prime_j]_\beta\), then\begin{equation}
        [T]_{\beta^\prime} = P^{-1}[T]_{\beta}P
    \end{equation}
    Alternatively, if \(U\alpha_j=\alpha_j^\prime\)\begin{equation}
        [T]_{\beta^\prime} = {[U]_\beta}^{-1}[T]_{\beta}[U]_\beta
    \end{equation}
    \item Let V be a finite-dimensional vector space over the field F, and let \(\beta=\{\alpha_1,\ldots,\,\alpha_n\}\) be a basis for V. Then there is a unique dual basis \(\beta^{*}=\{f_1,\ldots,\,f_n\}\) for \(V^{*}\) such that \(f_i(\alpha_j)=\delta_{ij}\). For each linear functional f on V we have \begin{equation}
        f = \sum_{i=1}^n f(\alpha_i)f_i
    \end{equation}
    and for each vector \(\alpha\) in V we have\begin{equation}
        \alpha = \sum_{i=1}^n f_i(\alpha)\alpha_i
    \end{equation}
    \item Let V be a finite-dimensional vector space over the field F, and let W be a subspace of V. Then\begin{equation}
        \dim W + \dim {W^0} = \dim V.
    \end{equation}
    \item Let V be a finite-dimensional vector space over the field F.\\ For each vector \(\alpha\) in V define\begin{equation}
        L_\alpha(f) = f(\alpha),\qquad f\in V^*
    \end{equation}
    The mapping \(\alpha \xrightarrow{} L_\alpha\) is then an isomorphism of V onto \(V^{**}\).
    \\ \tb{Corollary.} \\
    Let V be a finite-dimensional vector space over the field F. Each basis for \(V^*\) is the dual of some basis for V.\\
    \tb{Corollary.} \\
    Let V be a finite-dimensional vector space over the fiel F. If L is a linear functional on the dual space \(V^*\) of V, then there is a unique vector \(\alpha\) in V such that \begin{equation}
        L(f) = f(\alpha)
    \end{equation}
    for every f in \(V^*\)
\end{enumerate}
\end{CJK*}
\end{document}